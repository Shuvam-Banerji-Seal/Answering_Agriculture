{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8889e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, get_cosine_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba7243-da4e-4d0f-b51c-e68b62d5294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# Use environment variable for Hugging Face token\n",
    "hf_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "if hf_token:\n",
    "    login(hf_token)\n",
    "else:\n",
    "    print(\"Warning: HUGGINGFACE_TOKEN environment variable not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04fe3f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config=BitsAndBytesConfig(load_in_8bit=True)\n",
    "use_quant_config=True\n",
    "device='cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "# model_id=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model_id='meta-llama/Llama-3.2-1B'\n",
    "model_id='google/gemma-3-1b-it'\n",
    "tokenizer=AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id,padding_side='right')\n",
    "llm_model=AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                               torch_dtype=torch.bfloat16,\n",
    "                                               attn_implementation='eager',\n",
    "                                               quantization_config=quant_config if use_quant_config else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c6ca054-5b15-4c17-96c7-edca90ce4591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear8bitLt(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f076e6c-be1b-4366-9eff-f4c323e17b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping-:self_attn.q_proj.weight  typetorch.int8 \n",
      "skipping-:self_attn.k_proj.weight  typetorch.int8 \n",
      "skipping-:self_attn.v_proj.weight  typetorch.int8 \n",
      "skipping-:self_attn.o_proj.weight  typetorch.int8 \n",
      "available for grad self_attn.q_norm.weight and it's type torch.bfloat16\n",
      "available for grad self_attn.k_norm.weight and it's type torch.bfloat16\n",
      "skipping-:mlp.gate_proj.weight  typetorch.int8 \n",
      "skipping-:mlp.up_proj.weight  typetorch.int8 \n",
      "skipping-:mlp.down_proj.weight  typetorch.int8 \n",
      "available for grad input_layernorm.weight and it's type torch.bfloat16\n",
      "available for grad post_attention_layernorm.weight and it's type torch.bfloat16\n",
      "available for grad pre_feedforward_layernorm.weight and it's type torch.bfloat16\n",
      "available for grad post_feedforward_layernorm.weight and it's type torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "for param in llm_model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "\n",
    "\n",
    "for name,param in llm_model.model.layers[-1].named_parameters():\n",
    "    if param.is_floating_point():\n",
    "        print(f\"available for grad {name} and it's type {param.dtype}\")\n",
    "        param.requires_grad=True\n",
    "        \n",
    "    else:\n",
    "        print(f'skipping-:{name}  type{param.dtype} ')\n",
    "\n",
    "for param in llm_model.model.norm.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in llm_model.lm_head.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55361ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text= 'what is protein'\n",
    "# input_ids=tokenizer(text=text,return_tensors='pt')\n",
    "# input_ids={k:v.to(device) for k,v in input_ids.items()}\n",
    "# output=llm_model.generate(**input_ids,max_new_tokens=256)\n",
    "# ans=tokenizer.decode(output[0],skip_special_tokens=True,stream=True)\n",
    "# print('LLM REPLY-->',ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0977e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_dat={'Text':'do you think that these two are correlated in anyway. regardless now is the best time to “buy the dip” get in while ',\n",
    "         'Datatype':'Objective'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24af044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generator(data:dict):\n",
    "    text=data['Text']\n",
    "    datatype=data['Datatype']\n",
    "\n",
    "    prompt=f''' \n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are an Ai classification system that Read and understand the given Text in input and analyze the Text then classify the Content Type .\n",
    "NO EXPLANATIONS is required . YOU must choose from One of following Classes:\n",
    "Labelled class: Noise \n",
    "or Labelled class: Objective\n",
    "or Labelled class: Positive\n",
    "or Labelled class: Negative\n",
    "or Labelled class: QUESTION\n",
    "or Labelled class: Advertisement\n",
    "or Labelled class: Neutral sentiment\n",
    "or Labelled class: Miscellaneous\n",
    "Ensure Strictly that output is from above list.\n",
    "\n",
    "\n",
    "### Input:\n",
    "{text}\n",
    "\n",
    "### Response:\n",
    "Labelled class: {datatype}{tokenizer.eos_token}'''\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b28e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prompt_generator(data=dum_dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5382e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prompt_generator(data:dict):\n",
    "#     text=data['Text']\n",
    "#     prompt=[]\n",
    "\n",
    "#     prompt.append({\n",
    "#         'role':'system',\n",
    "#         'content':'''Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "# ### Instruction:\n",
    "# You are an Ai classification system that Read and understand the given Text in input and analyze the Text then classify the Content Type .\n",
    "# NO EXPLANATIONS is required . YOU must choose from One of following Classes:\n",
    "# Labelled class: Noise \n",
    "# or Labelled class: OBJECTIVE\n",
    "# or Labelled class: POSITIVE\n",
    "# or Labelled class: NEGATIVE\n",
    "# or Labelled class: QUESTION\n",
    "# or Labelled class: ADVERTISEMENT\n",
    "# or Labelled class: NEUTRAL SENTIMENT\n",
    "# or Labelled class: MISCELLANEOUS\n",
    "# Ensure Strictly that output is from above list.'''\n",
    "#     })\n",
    "#     prompt.append({\n",
    "#         'role':'user',\n",
    "#         'content':f'### Input:\\n{text}'\n",
    "#     })\n",
    "\n",
    "#     prompt.append({\n",
    "#         'role':'assistant',\n",
    "#         'content': 'Labelled class:'\n",
    "#     })\n",
    "#     return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b0d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4f65fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prompt_generator(data=dum_dat)+Response_generator(data=dum_dat)+tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35d8b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "class Instruction_datast(Dataset):\n",
    "    def __init__(self,csv_file,tokenizer,Max_length):\n",
    "        \n",
    "        self.file=pd.read_csv(csv_file)\n",
    "        self.data= self.file[['Text', 'Datatype']].to_dict(orient='records')\n",
    "\n",
    "        all_response=self._full_response(data=self.data)\n",
    "        self.encoded=[tokenizer.encode(response) for response in all_response]\n",
    " \n",
    "        if Max_length == None:\n",
    "            self.max_length=self._longest_length()\n",
    "        else:\n",
    "            self.max_length=Max_length\n",
    "        \n",
    "\n",
    "        encoded_text=tokenizer(all_response,return_tensors='pt',padding='max_length',truncation=True,max_length=self.max_length)\n",
    "        encoded_id=encoded_text['input_ids']\n",
    "        encoded_mask=encoded_text['attention_mask']\n",
    "             \n",
    "        self.input_ids=encoded_id[:,:-1]\n",
    "        self.input_mask=encoded_mask[:, :-1]\n",
    "        self.target_ids=encoded_id[: ,1:]\n",
    "        self.target_mask=encoded_mask[:, 1:]\n",
    "\n",
    "\n",
    "        self.target_compare = encoded_id[:, 1:].clone()\n",
    "        answer_texts = [\" \"+item[\"Datatype\"] + tokenizer.eos_token for item in self.data]\n",
    "        self.answer_token_ids =[tokenizer.encode(ans, add_special_tokens=False) for ans in answer_texts]\n",
    "\n",
    "        \n",
    "        for i in range(self.target_compare.shape[0]):\n",
    "            full = self.target_compare[i]  # shape: [seq_len\n",
    "            response=torch.tensor(self.encoded[i])\n",
    "            answer_ids = torch.tensor(self.answer_token_ids[i])\n",
    "\n",
    "            position=response.shape[0]-answer_ids.shape[0]-1\n",
    "            is_match = torch.equal(full[position:position + answer_ids.shape[0]],answer_ids)\n",
    "\n",
    "            if is_match:\n",
    "                mask = torch.full_like(full, -100)\n",
    "                mask[position:position + answer_ids.shape[0]] = full[position:position + answer_ids.shape[0]]\n",
    "                self.target_compare[i] = mask \n",
    "            else:\n",
    "                print('not found')\n",
    "        \n",
    "\n",
    "\n",
    "    def _longest_length(self):\n",
    "        return max((len(encoding) for encoding in self.encoded),default=0)\n",
    "\n",
    "    def _full_response(self,data:list[dict]):\n",
    "        full_response=[]\n",
    "        for dict in data:\n",
    "            prompt=prompt_generator(data=dict)\n",
    "            full_response.append(prompt)\n",
    "        return full_response\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file) \n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        ''' \n",
    "        return{\n",
    "            'input_ids':self.input_ids[index],\n",
    "            'attention_mask':self.input_mask[index]\n",
    "        },{\n",
    "            'input_ids':self.target_ids[index],\n",
    "            'attention_mask':self.target_mask[index]\n",
    "        }'''\n",
    "        return self.input_ids[index],self.target_compare[index]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bb0bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_list=dum_dat=[{'Text':'what is a stock market','Datatype':'Question'},\n",
    "                  {'Text':'nice too meet you','Datatype':'positive'},\n",
    "                  {'Text':'stock goes up','Datatype':'Question'}\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca2bd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test=Instruction_datast(csv_file='Vtwitter_test.csv',tokenizer=tokenizer,Max_length=None)\n",
    "dataset_train=Instruction_datast(csv_file='Vtwitter_train.csv',tokenizer=tokenizer,Max_length=None)\n",
    "dataset_val=Instruction_datast(csv_file='Vtwitter_validation.csv',tokenizer=tokenizer,Max_length=None)\n",
    "# check.input_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a04b8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 4\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e577a952-117d-4059-8f73-84a330d0d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "            if num_batches is not None and i >= num_batches:\n",
    "                break\n",
    "\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "\n",
    "            output = model(input_batch)\n",
    "            logits = output.logits  # shape [B, T, V]\n",
    "\n",
    "            # We assume next-token prediction → shift for accuracy\n",
    "            preds = logits.argmax(dim=-1)  # shape [B, T]\n",
    "            mask = (target_batch != -100)  # skip ignored positions\n",
    "\n",
    "            correct = (preds == target_batch) & mask\n",
    "            total_correct += correct.sum().item()\n",
    "            total_count += mask.sum().item()\n",
    "\n",
    "    return total_correct / total_count if total_count > 0 else float('nan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29ace153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    output = model(input_batch)  # assumed shape: [B, T, V]\n",
    "    logits=output.logits\n",
    "    if torch.isnan(logits).any():\n",
    "        print(\"⚠️ logits contain NaN!\")\n",
    "    if torch.isinf(logits).any():\n",
    "        print(\"⚠️ logits contain Inf!\")\n",
    "    loss = F.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)),  # [B*T, V]\n",
    "        target_batch.view(-1),             # [B*T]\n",
    "        ignore_index=-100                  # optional, if you use masked labels\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc4f073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_accuracy(dataloader,device,model,num_batches=None):\n",
    "    total_loss=0\n",
    "    if num_batches == None:\n",
    "        num_batches=len(dataloader)\n",
    "    else:\n",
    "        num_batches= min(num_batches,len(dataloader))\n",
    "    for i,(input_batch,target_batch) in enumerate(dataloader):\n",
    "        if i<num_batches:\n",
    "            # print(\"target min:\", target_batch.min().item(), \"max:\", target_batch.max().item())\n",
    "            loss= calc_loss_batch(input_batch=input_batch,target_batch=target_batch,model=model,device=device)\n",
    "            total_loss+= loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss/num_batches\n",
    "\n",
    "def evaluate_model(model, train_loader, vali_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_accuracy(train_loader, device, model, eval_iter)\n",
    "        vali_loss = calc_loss_accuracy(vali_loader, device, model, eval_iter)\n",
    "        train_acc = calc_accuracy_loader(train_loader, model, device, eval_iter)\n",
    "        val_acc = calc_accuracy_loader(vali_loader, model, device, eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, vali_loss, train_acc, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "688750cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter,scheduler):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            scheduler.step()\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss, train_acc, val_acc = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f} | \"\n",
    "                      f\"Train acc {train_acc*100:.2f}%, Val acc {val_acc*100:.2f}%\")\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2b749fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.583, Val loss 0.869 | Train acc 79.52%, Val acc 72.20%\n",
      "Ep 1 (Step 000040): Train loss 0.669, Val loss 0.826 | Train acc 74.63%, Val acc 74.63%\n",
      "Ep 1 (Step 000080): Train loss 0.655, Val loss 0.820 | Train acc 82.76%, Val acc 74.63%\n",
      "Ep 1 (Step 000120): Train loss 0.571, Val loss 0.871 | Train acc 77.56%, Val acc 73.17%\n",
      "Ep 1 (Step 000160): Train loss 0.550, Val loss 0.828 | Train acc 80.68%, Val acc 73.66%\n",
      "Ep 1 (Step 000200): Train loss 0.613, Val loss 0.811 | Train acc 82.13%, Val acc 75.12%\n",
      "Ep 1 (Step 000240): Train loss 0.647, Val loss 0.885 | Train acc 80.68%, Val acc 74.15%\n",
      "Ep 1 (Step 000280): Train loss 0.620, Val loss 0.799 | Train acc 78.57%, Val acc 76.59%\n",
      "Ep 1 (Step 000320): Train loss 0.582, Val loss 0.802 | Train acc 79.61%, Val acc 75.12%\n",
      "Ep 1 (Step 000360): Train loss 0.620, Val loss 0.847 | Train acc 83.25%, Val acc 75.12%\n",
      "Ep 1 (Step 000400): Train loss 0.523, Val loss 0.818 | Train acc 83.98%, Val acc 73.66%\n",
      "Ep 1 (Step 000440): Train loss 0.547, Val loss 0.842 | Train acc 81.25%, Val acc 75.12%\n",
      "Ep 1 (Step 000480): Train loss 0.565, Val loss 0.841 | Train acc 82.38%, Val acc 74.63%\n",
      "Training completed in 13.87 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(llm_model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 1\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "warmup_steps = int(0.1 * total_steps)  # 10% warmup\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    llm_model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=40, eval_iter=25,scheduler=scheduler\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face50a2-c9d6-44f5-ae62-2bbbc3824542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87758c1f-4158-4107-a156-68734f92edaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f8483-b06b-484f-8f87-f4cd34ff178c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
