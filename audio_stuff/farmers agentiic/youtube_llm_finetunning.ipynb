{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ce187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, get_cosine_schedule_with_warmup\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from accelerate import Accelerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a8e383-e51f-4f6e-9a07-ad58d6c0401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.9\n",
      "/home/bs_ms/sbs22ms076/fire/env/bin/python\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61c4e29-966e-4140-89b0-3caef9995913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e86a828d274deebdd856b64496ed4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config= BitsAndBytesConfig(load_in_8bit=True)\n",
    "use_quantization_config= True\n",
    "\n",
    "attention_implementation= 'eager'\n",
    "model_id='google/gemma-3-4b-it'\n",
    "tokenizer=AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id,padding_side=\"left\")\n",
    "llm_model=AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                        torch_dtype=torch.bfloat16,\n",
    "                                        quantization_config=quantization_config if use_quantization_config else None\n",
    "                                        )#attn_implementation=attention_implementation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81dcd5a2-25af-4a27-a5de-7030b3587862",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Gemma3Config' object has no attribute 'hidden_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Instead of trying to attach a head to a non-existent attribute,\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# we will create it as a separate module.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# The input to this head will be the output of the Gemma model.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Gemma 3B's hidden size is 2560.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m hidden_size = \u001b[43mllm_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel hidden size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m classification_head = nn.Sequential(\n\u001b[32m      9\u001b[39m     nn.LayerNorm(hidden_size),\n\u001b[32m     10\u001b[39m     nn.Dropout(\u001b[32m0.4\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     nn.Linear(in_features=\u001b[32m512\u001b[39m, out_features=\u001b[32m8\u001b[39m, dtype=torch.float32) \u001b[38;5;66;03m# 8 classes\u001b[39;00m\n\u001b[32m     18\u001b[39m ).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/configuration_utils.py:210\u001b[39m, in \u001b[36mPretrainedConfig.__getattribute__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    209\u001b[39m     key = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m)[key]\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Gemma3Config' object has no attribute 'hidden_size'"
     ]
    }
   ],
   "source": [
    "# # Instead of trying to attach a head to a non-existent attribute,\n",
    "# # we will create it as a separate module.\n",
    "# # The input to this head will be the output of the Gemma model.\n",
    "# # Gemma 3B's hidden size is 2560.\n",
    "# hidden_size = llm_model.config.hidden_size\n",
    "# print(f\"Model hidden size: {hidden_size}\")\n",
    "\n",
    "# classification_head = nn.Sequential(\n",
    "#     nn.LayerNorm(hidden_size),\n",
    "#     nn.Dropout(0.4),\n",
    "#     nn.Linear(in_features=hidden_size, out_features=1024, dtype=torch.float32),\n",
    "#     nn.GELU(),\n",
    "#     nn.Dropout(0.4),\n",
    "#     nn.Linear(in_features=1024, out_features=512, dtype=torch.float32),\n",
    "#     nn.GELU(),\n",
    "#     nn.Dropout(0.4),\n",
    "#     nn.Linear(in_features=512, out_features=8, dtype=torch.float32) # 8 classes\n",
    "# ).to(device)\n",
    "\n",
    "# # Freeze the original LLM parameters\n",
    "# for param in llm_model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d922615b-c5ea-4913-b98f-e15ab92e5c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "encoded_text=['kya baaat','sab changa si','moj kardi bete waah bete waah                               ']\n",
    "max((len(encoding) for encoding in encoded_text),default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08ef824",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "class Spam_dataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, Max_length=None):\n",
    "        self.data=pd.read_csv(csv_file)\n",
    "        self.encoded_text=[tokenizer.encode(text) for text in self.data['Text']]\n",
    "        texts = list(self.data['Text'])\n",
    "\n",
    "        if Max_length == None:\n",
    "            self.max_length=self._longest_length()\n",
    "        else:\n",
    "            self.max_length=Max_length\n",
    "\n",
    "        self.tokenized = tokenizer(\n",
    "            texts,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        self.labels = torch.tensor(self.data['Datatype'].values, dtype=torch.long)\n",
    "    def _longest_length(self):\n",
    "        return max((len(encoding) for encoding in self.encoded_text),default=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label=self.data.iloc[index]['Datatype']\n",
    "        return {\n",
    "            'input_ids': self.tokenized['input_ids'][index],\n",
    "            'attention_mask': self.tokenized['attention_mask'][index]\n",
    "        }, torch.tensor(label,dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43fc671e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e47ed8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=Spam_dataset(csv_file='youtube_train.csv',tokenizer=tokenizer,Max_length=None)\n",
    "test_dataset=Spam_dataset(csv_file='youtube_test.csv',tokenizer=tokenizer,Max_length=train_dataset.max_length)\n",
    "vali_dataset=Spam_dataset(csv_file='youtube_validation.csv',tokenizer=tokenizer,Max_length=train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "188c5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(693)\n",
    "worker_size=0\n",
    "batch_size=4\n",
    "\n",
    "train_loader= DataLoader(dataset=train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=worker_size,\n",
    "                         shuffle=True,\n",
    "                         drop_last=True\n",
    "                         )\n",
    "vali_loader= DataLoader(dataset=vali_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=worker_size,\n",
    "                         shuffle=True,\n",
    "                         drop_last=True\n",
    "                         )\n",
    "test_loader= DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=worker_size,\n",
    "                         shuffle=True,\n",
    "                         drop_last=True\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a17f3a-f23a-408a-8408-df2afc010ab2",
   "metadata": {},
   "source": [
    "for 1 billion model it's syntax is llm_model.lm_head.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aa86c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in llm_model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "# llm_model.out_head=nn.Sequential(nn.LayerNorm(2560),nn.Dropout(p=0.2),nn.Linear(in_features=2560 ,out_features=8,dtype=torch.float32)).to(device)\n",
    "llm_model.language_model.out_head=nn.Sequential(nn.LayerNorm(2560),nn.Dropout(0.4),nn.Linear(in_features=2560 ,out_features=1024,dtype=torch.float32),nn.GELU()\n",
    "                                 ,nn.Dropout(0.4),nn.Linear(in_features=1024 ,out_features=512,dtype=torch.float32),nn.GELU(),\n",
    "                                  nn.Dropout(0.4),nn.Linear(in_features=512 ,out_features=8,dtype=torch.float32)).to(device)\n",
    "# print(\"head weight dtype:\", llm_model.out_head.weight.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "855c7f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping-:self_attn.q_proj.weight  typetorch.int8 \n",
      "skipping-:self_attn.k_proj.weight  typetorch.int8 \n",
      "skipping-:self_attn.v_proj.weight  typetorch.int8 \n",
      "skipping-:self_attn.o_proj.weight  typetorch.int8 \n",
      "available for grad self_attn.q_norm.weight and it's type torch.bfloat16\n",
      "available for grad self_attn.k_norm.weight and it's type torch.bfloat16\n",
      "skipping-:mlp.gate_proj.weight  typetorch.int8 \n",
      "skipping-:mlp.up_proj.weight  typetorch.int8 \n",
      "skipping-:mlp.down_proj.weight  typetorch.int8 \n",
      "available for grad input_layernorm.weight and it's type torch.bfloat16\n",
      "available for grad post_attention_layernorm.weight and it's type torch.bfloat16\n",
      "available for grad pre_feedforward_layernorm.weight and it's type torch.bfloat16\n",
      "available for grad post_feedforward_layernorm.weight and it's type torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "for name,param in llm_model.language_model.model.layers[-1].named_parameters():\n",
    "    if param.is_floating_point():\n",
    "        print(f\"available for grad {name} and it's type {param.dtype}\")\n",
    "        param.requires_grad=True\n",
    "        \n",
    "    else:\n",
    "        print(f'skipping-:{name}  type{param.dtype} ')\n",
    "\n",
    "for param in llm_model.language_model.model.norm.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in llm_model.language_model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in llm_model.language_model.out_head.parameters(): #------>tried training my out head when arcihtecture was layer norm(1156),dropout,(1156,576),gelu,dropout,(576,8)\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebab26cb-d5e5-4a72-a17b-197790a92c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForConditionalGeneration(\n",
       "  (vision_tower): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(4096, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear8bitLt(in_features=1152, out_features=1152, bias=True)\n",
       "              (v_proj): Linear8bitLt(in_features=1152, out_features=1152, bias=True)\n",
       "              (q_proj): Linear8bitLt(in_features=1152, out_features=1152, bias=True)\n",
       "              (out_proj): Linear8bitLt(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear8bitLt(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "    (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  )\n",
       "  (language_model): Gemma3ForCausalLM(\n",
       "    (model): Gemma3TextModel(\n",
       "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-33): 34 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear8bitLt(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear8bitLt(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear8bitLt(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear8bitLt(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear8bitLt(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear8bitLt(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear8bitLt(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "      (rotary_emb): Gemma3RotaryEmbedding()\n",
       "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       "    (out_head): Sequential(\n",
       "      (0): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Dropout(p=0.4, inplace=False)\n",
       "      (2): Linear(in_features=2560, out_features=1024, bias=True)\n",
       "      (3): GELU(approximate='none')\n",
       "      (4): Dropout(p=0.4, inplace=False)\n",
       "      (5): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (6): GELU(approximate='none')\n",
       "      (7): Dropout(p=0.4, inplace=False)\n",
       "      (8): Linear(in_features=512, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "294cc900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,  3886,   613,   659,  6762, 31716, 10070, 18885]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bs_ms/.local/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last token max: 30.875 min: -18.0\n",
      "Logits: tensor([[ 0.1144, -0.0674,  0.0183,  0.0104,  0.2213, -0.2454, -0.1271, -0.1769]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Logits shape: torch.Size([1, 8])\n",
      "tensor([4], device='cuda:0')\n",
      "tensor([[0.1431, 0.1193, 0.1300, 0.1290, 0.1593, 0.0999, 0.1124, 0.1070]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text= 'waht are protein shake brother ??'\n",
    "input_id= tokenizer(text,return_tensors='pt')\n",
    "\n",
    "# embed_device = llm_model.language_model.model.get_input_embeddings().weight.device\n",
    "input_id = {k: v.to(device) for k, v in input_id.items()}\n",
    "print(input_id)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = llm_model.language_model.model(**input_id)  \n",
    "    last_token = outputs.last_hidden_state[:, -1, :]  \n",
    "    \n",
    "print(\"Last token max:\", last_token.max().item(), \"min:\", last_token.min().item())\n",
    "logits = llm_model.language_model.out_head(last_token.float())\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "norm=torch.softmax(logits,dim=-1)        #you can keep dim=-1 for same output \n",
    "maxi=torch.argmax(logits,dim=-1) #you can keep dim=1 same output \n",
    "print(maxi)\n",
    "print(norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67da3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader( data_loader, model, device,num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "            target_batch = target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # print(\"input_ids:\", input_batch[\"input_ids\"])\n",
    "                outputs = llm_model.language_model.model(**input_batch)\n",
    "                last_token = outputs.last_hidden_state[:, -1, :]\n",
    "                # print(\"last_hidden_state\",last_token)\n",
    "                logits = llm_model.language_model.out_head(last_token.float())\n",
    "                # print('logits',logits)  \n",
    "            predicted_labels=torch.argmax(logits,dim=1)\n",
    "            # print('predicted_labels',predicted_labels)\n",
    "            num_examples += logits.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bd52a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model=llm_model, device=device, num_batches=8)\n",
    "train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "573a2793-e32f-4278-8674-390de9f776ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1310, 0.2090, 0.5260, 1.9506, 0.7675, 1.0762, 3.1210, 0.2188],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label_penalty=[400,230,70,16,40,30,10,220]\n",
    "label_penalty=[715,448,178,48,122,87,30,428]\n",
    "label_tens=torch.tensor(label_penalty,dtype=torch.float)\n",
    "weights=1/label_tens\n",
    "weights=(weights/weights.sum())*len(label_penalty)\n",
    "weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f76a374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(input_batch,target_batch,model,device,weights=weights):\n",
    "    input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "    target_batch = target_batch.to(device)\n",
    "    # print(\"target shape:\", target_batch.shape)\n",
    "    # print(\"target min:\", target_batch.min().item(), \"max:\", target_batch.max().item())\n",
    "    outputs = model.language_model.model(**input_batch)\n",
    "    last_token = outputs.last_hidden_state[:, -1, :]\n",
    "    # print(\"Last token max:\", last_token.max().item(), \"min:\", last_token.min().item())\n",
    "    if torch.isnan(last_token).any():\n",
    "        print(\"⚠️ last_token has NaNs!\")\n",
    "    if torch.isinf(last_token).any():\n",
    "        print(\"⚠️ last_token has Infs!\")\n",
    "    logits = model.language_model.out_head(last_token.float())\n",
    "    # print(\"logits shape:\", logits.shape)\n",
    "    if torch.isnan(logits).any():\n",
    "        print(\"⚠️ logits contain NaN!\")\n",
    "    if torch.isinf(logits).any():\n",
    "        print(\"⚠️ logits contain Inf!\")\n",
    "    weights=weights.to(device=device)\n",
    "    \n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch,weight=weights)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9373efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_accuracy(dataloader,device,model,num_batches=None):\n",
    "    total_loss=0\n",
    "    if num_batches == None:\n",
    "        num_batches=len(dataloader)\n",
    "    else:\n",
    "        num_batches= min(num_batches,len(dataloader))\n",
    "    for i,(input_batch,target_batch) in enumerate(dataloader):\n",
    "        if i<num_batches:\n",
    "            # print(\"target min:\", target_batch.min().item(), \"max:\", target_batch.max().item())\n",
    "            loss= loss_func(input_batch=input_batch,target_batch=target_batch,model=model,device=device)\n",
    "            total_loss+= loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss/num_batches\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "163f3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, vali_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_accuracy(dataloader=train_loader,device=device,model=model, num_batches=eval_iter)\n",
    "        vali_loss = calc_loss_accuracy(dataloader=vali_loader,device=device,model=model, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, vali_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f587533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_classifier_simple(model, train_loader, vali_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter,scheduler,best_val_accuracy=0.75):\n",
    "    \n",
    "    train_losses, vali_losses, train_accs, vali_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() \n",
    "            loss = loss_func(input_batch=input_batch,target_batch=target_batch,model=model,device=device)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            examples_seen += input_batch['input_ids'].shape[0]\n",
    "            global_step += 1\n",
    "\n",
    "           \n",
    "            if global_step % eval_freq == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                train_loss, vali_loss = evaluate_model(\n",
    "                    model=model,train_loader= train_loader, vali_loader=vali_loader, device=device, eval_iter=eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                vali_losses.append(vali_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {vali_loss:.3f}\")\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "                vali_accuracy = calc_accuracy_loader(vali_loader, model, device, num_batches=eval_iter)\n",
    "                print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "                print(f\"Validation accuracy: {vali_accuracy*100:.2f}%\")\n",
    "                train_accs.append(train_accuracy)\n",
    "                vali_accs.append(vali_accuracy)\n",
    "\n",
    "                if vali_accuracy >= best_val_accuracy:\n",
    "                    if (train_accuracy - vali_accuracy) <=0.05:\n",
    "                        best_val_accuracy = vali_accuracy \n",
    "                        torch.save({\n",
    "                            'model_state_dict': llm_model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'scheduler_state_dict': scheduler.state_dict(),\n",
    "                        }, f'youtube_classifier_model{int(train_accuracy*100)}_{int(vali_accuracy*100)}_.pt')\n",
    "                        print(f\"Model saved with improved accuracy: {vali_accuracy:.4f}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return train_losses, vali_losses, train_accs, vali_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5261b84",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 22.43 GiB is allocated by PyTorch, and 759.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     10\u001b[39m warmup_steps = \u001b[38;5;28mint\u001b[39m(\u001b[32m0.15\u001b[39m * total_steps)  \u001b[38;5;66;03m# 10% warmup\u001b[39;00m\n\u001b[32m     11\u001b[39m scheduler = get_cosine_schedule_with_warmup(\n\u001b[32m     12\u001b[39m     optimizer,\n\u001b[32m     13\u001b[39m     num_warmup_steps=warmup_steps,\n\u001b[32m     14\u001b[39m     num_training_steps=total_steps\n\u001b[32m     15\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m train_losses, vali_losses, train_accs, vali_accs, examples_seen = \u001b[43mtrain_classifier_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvali_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvali_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m end_time = time.time()\n\u001b[32m     23\u001b[39m execution_time_minutes = (end_time - start_time) / \u001b[32m60\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_classifier_simple\u001b[39m\u001b[34m(model, train_loader, vali_loader, optimizer, device, num_epochs, eval_freq, eval_iter, scheduler, best_val_accuracy)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m input_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     12\u001b[39m     optimizer.zero_grad() \n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     loss = \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     15\u001b[39m     loss.backward() \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mloss_func\u001b[39m\u001b[34m(input_batch, target_batch, model, device, weights)\u001b[39m\n\u001b[32m      3\u001b[39m target_batch = target_batch.to(device)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# print(\"target shape:\", target_batch.shape)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# print(\"target min:\", target_batch.min().item(), \"max:\", target_batch.max().item())\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlanguage_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m last_token = outputs.last_hidden_state[:, -\u001b[32m1\u001b[39m, :]\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# print(\"Last token max:\", last_token.max().item(), \"min:\", last_token.min().item())\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:722\u001b[39m, in \u001b[36mGemma3TextModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, last_cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    708\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    709\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    710\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    719\u001b[39m         last_cache_position,\n\u001b[32m    720\u001b[39m     )\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m722\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlast_cache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlast_cache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:420\u001b[39m, in \u001b[36mGemma3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, last_cache_position, **kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    418\u001b[39m     position_embeddings = position_embeddings_global\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m    432\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:342\u001b[39m, in \u001b[36mGemma3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    340\u001b[39m     \u001b[38;5;66;03m# backwards compatibility\u001b[39;00m\n\u001b[32m    341\u001b[39m     attention_mask = attention_mask.to(query_states)\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    355\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.jit.is_tracing() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(is_causal, torch.Tensor):\n\u001b[32m     52\u001b[39m     is_causal = is_causal.item()\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 62.06 MiB is free. Including non-PyTorch memory, this process has 23.37 GiB memory in use. Of the allocated memory 22.43 GiB is allocated by PyTorch, and 759.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(693)\n",
    "\n",
    "optimizer = torch.optim.AdamW(llm_model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 3\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "warmup_steps = int(0.15 * total_steps)  # 10% warmup\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "train_losses, vali_losses, train_accs, vali_accs, examples_seen = train_classifier_simple(\n",
    "    model=llm_model, train_loader=train_loader, vali_loader=vali_loader, optimizer=optimizer, device=device,\n",
    "    num_epochs=num_epochs, eval_freq=40, eval_iter=8,scheduler=scheduler\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80083898-333b-481a-a031-d66edf2a492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([name for name, _ in llm_model.named_parameters() if 'out_head' in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd131cb-bae2-4bed-8984-6c14525cb9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27780270436c43deb01713c2dfc18b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"Gemma3ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"boi_token_index\": 255999,\n",
      "  \"eoi_token_index\": 256000,\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    106\n",
      "  ],\n",
      "  \"image_token_index\": 262144,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"mm_tokens_per_image\": 256,\n",
      "  \"model_type\": \"gemma3\",\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": false,\n",
      "    \"_load_in_8bit\": true,\n",
      "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"fp4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": false,\n",
      "    \"load_in_8bit\": true,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"text_config\": {\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"attn_logit_softcapping\": null,\n",
      "    \"cache_implementation\": \"hybrid\",\n",
      "    \"final_logit_softcapping\": null,\n",
      "    \"head_dim\": 256,\n",
      "    \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "    \"hidden_size\": 2560,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 10240,\n",
      "    \"max_position_embeddings\": 131072,\n",
      "    \"model_type\": \"gemma3_text\",\n",
      "    \"num_attention_heads\": 8,\n",
      "    \"num_hidden_layers\": 34,\n",
      "    \"num_key_value_heads\": 4,\n",
      "    \"query_pre_attn_scalar\": 256,\n",
      "    \"rms_norm_eps\": 1e-06,\n",
      "    \"rope_local_base_freq\": 10000.0,\n",
      "    \"rope_scaling\": {\n",
      "      \"factor\": 8.0,\n",
      "      \"rope_type\": \"linear\"\n",
      "    },\n",
      "    \"rope_theta\": 1000000.0,\n",
      "    \"sliding_window\": 1024,\n",
      "    \"sliding_window_pattern\": 6,\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 262208\n",
      "  },\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"vision_config\": {\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "    \"hidden_size\": 1152,\n",
      "    \"image_size\": 896,\n",
      "    \"intermediate_size\": 4304,\n",
      "    \"layer_norm_eps\": 1e-06,\n",
      "    \"model_type\": \"siglip_vision_model\",\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 27,\n",
      "    \"patch_size\": 14,\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"vision_use_head\": false\n",
      "  }\n",
      "}\n",
      "\n",
      "=======================================\n",
      "           STARTING TRAINING           \n",
      "=======================================\n",
      "\n",
      "--- Starting Epoch 1/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bs_ms/.local/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Epoch 1, Step 50/875] Training batch loss: 1.9114\n",
      "  [Epoch 1, Step 100/875] Training batch loss: 1.9048\n",
      "  [Epoch 1, Step 150/875] Training batch loss: 1.6292\n",
      "  [Epoch 1, Step 200/875] Training batch loss: 1.9586\n",
      "  [Epoch 1, Step 250/875] Training batch loss: 1.2660\n",
      "  [Epoch 1, Step 300/875] Training batch loss: 0.9970\n",
      "  [Epoch 1, Step 350/875] Training batch loss: 1.1883\n",
      "  [Epoch 1, Step 400/875] Training batch loss: 1.1585\n",
      "  [Epoch 1, Step 450/875] Training batch loss: 1.2181\n",
      "  [Epoch 1, Step 500/875] Training batch loss: 1.1943\n",
      "  [Epoch 1, Step 550/875] Training batch loss: 0.6756\n",
      "  [Epoch 1, Step 600/875] Training batch loss: 0.7232\n",
      "  [Epoch 1, Step 650/875] Training batch loss: 0.7120\n",
      "  [Epoch 1, Step 700/875] Training batch loss: 0.7846\n",
      "  [Epoch 1, Step 750/875] Training batch loss: 0.7457\n",
      "  [Epoch 1, Step 800/875] Training batch loss: 0.7083\n",
      "  [Epoch 1, Step 850/875] Training batch loss: 1.0695\n",
      "--- Finished training for Epoch 1 ---\n",
      "\n",
      "--- Starting evaluation for Epoch 1 ---\n",
      "--- Evaluation complete for Epoch 1 ---\n",
      "  => Epoch 1 Results: Val Loss: 0.9174, Val Accuracy: 0.5495\n",
      "  ✨ New best accuracy: 0.5495. Saving model... ✨\n",
      "  ✅ Model saved.\n",
      "\n",
      "--- Starting Epoch 2/3 ---\n",
      "  [Epoch 2, Step 50/875] Training batch loss: 1.9178\n",
      "  [Epoch 2, Step 100/875] Training batch loss: 0.5372\n",
      "  [Epoch 2, Step 150/875] Training batch loss: 0.6355\n",
      "  [Epoch 2, Step 200/875] Training batch loss: 0.8335\n",
      "  [Epoch 2, Step 250/875] Training batch loss: 0.8501\n",
      "  [Epoch 2, Step 300/875] Training batch loss: 1.6873\n",
      "  [Epoch 2, Step 350/875] Training batch loss: 0.6986\n",
      "  [Epoch 2, Step 400/875] Training batch loss: 2.0300\n",
      "  [Epoch 2, Step 450/875] Training batch loss: 0.8492\n",
      "  [Epoch 2, Step 500/875] Training batch loss: 0.8052\n",
      "  [Epoch 2, Step 550/875] Training batch loss: 0.5919\n",
      "  [Epoch 2, Step 600/875] Training batch loss: 0.7123\n",
      "  [Epoch 2, Step 650/875] Training batch loss: 0.6659\n",
      "  [Epoch 2, Step 700/875] Training batch loss: 0.4068\n",
      "  [Epoch 2, Step 750/875] Training batch loss: 0.7689\n",
      "  [Epoch 2, Step 800/875] Training batch loss: 0.6206\n",
      "  [Epoch 2, Step 850/875] Training batch loss: 2.1831\n",
      "--- Finished training for Epoch 2 ---\n",
      "\n",
      "--- Starting evaluation for Epoch 2 ---\n",
      "--- Evaluation complete for Epoch 2 ---\n",
      "  => Epoch 2 Results: Val Loss: 0.8664, Val Accuracy: 0.5756\n",
      "  ✨ New best accuracy: 0.5756. Saving model... ✨\n",
      "  ✅ Model saved.\n",
      "\n",
      "--- Starting Epoch 3/3 ---\n",
      "  [Epoch 3, Step 50/875] Training batch loss: 0.2742\n",
      "  [Epoch 3, Step 100/875] Training batch loss: 0.9501\n",
      "  [Epoch 3, Step 150/875] Training batch loss: 1.0301\n",
      "  [Epoch 3, Step 200/875] Training batch loss: 1.7008\n",
      "  [Epoch 3, Step 250/875] Training batch loss: 1.0655\n",
      "  [Epoch 3, Step 300/875] Training batch loss: 1.0217\n",
      "  [Epoch 3, Step 350/875] Training batch loss: 0.6635\n",
      "  [Epoch 3, Step 400/875] Training batch loss: 0.5695\n",
      "  [Epoch 3, Step 450/875] Training batch loss: 0.6624\n",
      "  [Epoch 3, Step 500/875] Training batch loss: 0.8594\n",
      "  [Epoch 3, Step 550/875] Training batch loss: 0.7162\n",
      "  [Epoch 3, Step 600/875] Training batch loss: 0.3658\n",
      "  [Epoch 3, Step 650/875] Training batch loss: 0.6651\n",
      "  [Epoch 3, Step 700/875] Training batch loss: 1.9733\n",
      "  [Epoch 3, Step 750/875] Training batch loss: 1.6809\n",
      "  [Epoch 3, Step 800/875] Training batch loss: 1.8068\n",
      "  [Epoch 3, Step 850/875] Training batch loss: 0.5539\n",
      "--- Finished training for Epoch 3 ---\n",
      "\n",
      "--- Starting evaluation for Epoch 3 ---\n",
      "--- Evaluation complete for Epoch 3 ---\n",
      "  => Epoch 3 Results: Val Loss: 0.8546, Val Accuracy: 0.5756\n",
      "\n",
      "=======================================\n",
      "           TRAINING COMPLETE           \n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "# train_spam.py\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, get_cosine_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from accelerate import Accelerator # You already have this!\n",
    "import time\n",
    "\n",
    "# ==========================================================\n",
    "# 1. Initialize Accelerator at the beginning of your script\n",
    "# ==========================================================\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# The device will be handled by Accelerator\n",
    "device = accelerator.device\n",
    "\n",
    "# --- Model and Tokenizer Loading (No changes here, but good) ---\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "use_quantization_config = True\n",
    "model_id = 'google/gemma-3-4b-it'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model, but DO NOT move to device with .to(device) yet\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config if use_quantization_config else None,\n",
    "    # For better performance on your RTX cards, enable flash attention\n",
    "    attn_implementation=\"eager\" \n",
    ")\n",
    "print(llm_model.config)\n",
    "# --- Your Custom Head ---\n",
    "# NEW, correct line:\n",
    "# NEW, correct line for your multimodal model:\n",
    "hidden_size = llm_model.config.text_config.hidden_size\n",
    "# Add this line to your code to see all available config options\n",
    "\n",
    "classification_head = nn.Sequential(\n",
    "    nn.LayerNorm(hidden_size),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(in_features=hidden_size, out_features=1024, dtype=torch.float32),\n",
    "    nn.GELU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(in_features=1024, out_features=512, dtype=torch.float32),\n",
    "    nn.GELU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(in_features=512, out_features=8, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "# Freeze LLM\n",
    "for param in llm_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --- Dataset and Dataloader (No changes to the class itself) ---\n",
    "class Spam_dataset(Dataset):\n",
    "    # ... your class code is fine ...\n",
    "    def __init__(self, csv_file, tokenizer, Max_length=None):\n",
    "        self.data=pd.read_csv(csv_file)\n",
    "        texts = list(self.data['Text'])\n",
    "\n",
    "        if Max_length == None:\n",
    "            encoded_text=[tokenizer.encode(text) for text in self.data['Text']]\n",
    "            self.max_length = max((len(encoding) for encoding in encoded_text),default=0)\n",
    "        else:\n",
    "            self.max_length=Max_length\n",
    "\n",
    "        self.tokenized = tokenizer(\n",
    "            texts, padding='max_length', truncation=True,\n",
    "            max_length=self.max_length, return_tensors='pt'\n",
    "        )\n",
    "        self.labels = torch.tensor(self.data['Datatype'].values, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # The label is now part of the dictionary for easier handling with accelerate\n",
    "        return {\n",
    "            'input_ids': self.tokenized['input_ids'][index],\n",
    "            'attention_mask': self.tokenized['attention_mask'][index],\n",
    "            'labels': self.labels[index]\n",
    "        }\n",
    "\n",
    "train_dataset = Spam_dataset(csv_file='youtube_train.csv', tokenizer=tokenizer, Max_length=None)\n",
    "vali_dataset = Spam_dataset(csv_file='youtube_validation.csv', tokenizer=tokenizer, Max_length=train_dataset.max_length)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True)\n",
    "vali_loader = DataLoader(dataset=vali_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# --- Optimizer and Scheduler ---\n",
    "# We combine parameters from the head. Only these will be updated.\n",
    "optimizer = torch.optim.AdamW(classification_head.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 3\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "warmup_steps = int(0.15 * total_steps)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# 2. Use accelerator.prepare()\n",
    "# This is the magic step. It wraps your models, optimizer, and dataloaders.\n",
    "# ==========================================================\n",
    "llm_model, classification_head, optimizer, train_loader, vali_loader, scheduler = accelerator.prepare(\n",
    "    llm_model, classification_head, optimizer, train_loader, vali_loader, scheduler\n",
    ")\n",
    "\n",
    "# --- Loss Weights (Don't move to device manually) ---\n",
    "label_penalty = [715, 448, 178, 48, 122, 87, 30, 428]\n",
    "label_tens = torch.tensor(label_penalty, dtype=torch.float)\n",
    "weights = 1 / label_tens\n",
    "weights = (weights / weights.sum()) * len(label_penalty)\n",
    "# Move weights to the correct device inside the training loop\n",
    "\n",
    "# (Assuming your accelerator.prepare() call is done before this)\n",
    "\n",
    "# --- Define logging frequency ---\n",
    "log_interval = 50  # Print training progress every 50 steps\n",
    "\n",
    "# --- Training Loop ---\n",
    "best_val_accuracy = 0.0\n",
    "accelerator.print(\"=======================================\")\n",
    "accelerator.print(\"           STARTING TRAINING           \")\n",
    "accelerator.print(\"=======================================\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    accelerator.print(f\"\\n--- Starting Epoch {epoch+1}/{num_epochs} ---\")\n",
    "    \n",
    "    # --- Set models to correct mode ---\n",
    "    llm_model.eval()  # Base model is always in eval mode as it's frozen\n",
    "    classification_head.train()\n",
    "    \n",
    "    # --- Training Phase for the current epoch ---\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = llm_model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            last_hidden_state = outputs.hidden_states[-1]\n",
    "            last_token_hidden_state = last_hidden_state[:, -1, :]\n",
    "            \n",
    "        logits = classification_head(last_token_hidden_state.float())\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels, weight=weights.to(device))\n",
    "        \n",
    "        # Print progress periodically\n",
    "        if step % log_interval == 0 and step > 0:\n",
    "            accelerator.print(f\"  [Epoch {epoch+1}, Step {step}/{len(train_loader)}] Training batch loss: {loss.item():.4f}\")\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    accelerator.print(f\"--- Finished training for Epoch {epoch+1} ---\")\n",
    "\n",
    "    # --- Evaluation Phase for the current epoch ---\n",
    "    accelerator.print(f\"\\n--- Starting evaluation for Epoch {epoch+1} ---\")\n",
    "    llm_model.eval()\n",
    "    classification_head.eval()\n",
    "    total_eval_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(vali_loader):\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "\n",
    "            outputs = llm_model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            last_hidden_state = outputs.hidden_states[-1]\n",
    "            last_token_hidden_state = last_hidden_state[:, -1, :]\n",
    "\n",
    "            logits = classification_head(last_token_hidden_state.float())\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels, weight=weights.to(device))\n",
    "            \n",
    "            total_eval_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            gathered_preds = accelerator.gather(preds)\n",
    "            gathered_labels = accelerator.gather(labels)\n",
    "            \n",
    "            correct_predictions += (gathered_preds == gathered_labels).sum().item()\n",
    "            total_samples += gathered_labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(vali_loader)\n",
    "    val_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    # Print the final evaluation results for the epoch\n",
    "    accelerator.print(f\"--- Evaluation complete for Epoch {epoch+1} ---\")\n",
    "    accelerator.print(f\"  => Epoch {epoch+1} Results: Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Check for improvement and save the model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        accelerator.print(f\"  ✨ New best accuracy: {best_val_accuracy:.4f}. Saving model... ✨\")\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_head = accelerator.unwrap_model(classification_head)\n",
    "        accelerator.save(unwrapped_head.state_dict(), f\"youtube_classifier_head_acc_{val_accuracy:.2f}.pt\")\n",
    "        accelerator.print(\"  ✅ Model saved.\")\n",
    "\n",
    "accelerator.print(\"\\n=======================================\")\n",
    "accelerator.print(\"           TRAINING COMPLETE           \")\n",
    "accelerator.print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8830a24f-2e37-4a14-aa44-458a7b8800ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
