"""
IndicAgri Frontend Server - Integrated with Existing Backend Modules
"""

from fastapi import FastAPI, Request, HTTPException, UploadFile, File
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, JSONResponse
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import logging
import asyncio
import json
import sys
import os
from datetime import datetime
import traceback

# Add your project root to Python path
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(PROJECT_ROOT)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="IndicAgri Frontend", 
    description="Agricultural AI Assistant Frontend",
    version="1.0.0"
)

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

# Templates
templates = Jinja2Templates(directory="templates")

# Initialize backend connections (lazy loading)
_embedding_system = None
_sub_query_generator = None

def get_embedding_system():
    """Lazy load embedding system"""
    global _embedding_system
    if _embedding_system is None:
        try:
            from embedding_generator.src.embedding_system import EmbeddingSystem
            _embedding_system = EmbeddingSystem()
            logger.info("Embedding system initialized")
        except Exception as e:
            logger.error(f"Failed to initialize embedding system: {e}")
            _embedding_system = None
    return _embedding_system

def get_sub_query_generator():
    """Lazy load sub query generator"""
    global _sub_query_generator
    if _sub_query_generator is None:
        try:
            from sub_query_generation.main import SubQueryProcessor
            _sub_query_generator = SubQueryProcessor()
            logger.info("Sub-query generator initialized")
        except Exception as e:
            logger.error(f"Failed to initialize sub-query generator: {e}")
            _sub_query_generator = None
    return _sub_query_generator

# Request/Response Models
class QueryRequest(BaseModel):
    query: str
    input_type: str = "text"
    language: str = "en"  # Default language changed to English
    user_location: Optional[str] = None
    session_id: Optional[str] = None

class Source(BaseModel):
    title: str
    url: str = "#"
    relevance: float
    content_preview: Optional[str] = None

class QueryResponse(BaseModel):
    answer: str
    sources: List[Source] = []
    confidence: float
    processing_time: Optional[float] = None
    sub_queries: Optional[List[str]] = []
    error: Optional[str] = None

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """Serve the main application page"""
    # Get language from query params, default to English
    lang = request.query_params.get("lang", "en")
    return templates.TemplateResponse("index.html", {
        "request": request,
        "app_name": "IndicAgri",
        "version": "1.0.0",
        "lang": lang
    })

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy", 
        "timestamp": datetime.now().isoformat(),
        "services": {
            "embedding_system": get_embedding_system() is not None,
            "sub_query_generator": get_sub_query_generator() is not None
        }
    }

@app.post("/api/v1/query", response_model=QueryResponse)
async def process_query(query_request: QueryRequest):
    """
    Process agricultural query using your existing backend modules
    """
    start_time = datetime.now()
    try:
        logger.info(f"Processing query: {query_request.query[:100]}...")
        # Use language from request
        language = query_request.language or "en"
        # Try to use your actual backend systems
        if get_sub_query_generator() and get_embedding_system():
            result = await process_with_backend(query_request)
        else:
            # Fallback to enhanced mock responses
            result = await get_enhanced_mock_response(query_request)
        # Calculate processing time
        processing_time = (datetime.now() - start_time).total_seconds()
        result.processing_time = processing_time
        logger.info(f"Query processed in {processing_time:.2f}s")
        return result
    except Exception as e:
        logger.error(f"Error processing query: {str(e)}")
        logger.error(traceback.format_exc())
        # Error message in English
        return QueryResponse(
            answer="Sorry, your query could not be processed due to a technical issue. Please try again.",
            sources=[],
            confidence=0.0,
            processing_time=(datetime.now() - start_time).total_seconds(),
            error=str(e)
        )

async def process_with_backend(query_request: QueryRequest) -> QueryResponse:
    """Process query using your actual backend systems"""
    try:
        # Generate sub-queries using your existing system
        sub_query_gen = get_sub_query_generator()
        sub_queries = []
        
        if sub_query_gen:
            # Use your actual sub-query generation
            sub_queries = await asyncio.to_thread(
                sub_query_gen.generate_sub_queries, 
                query_request.query
            )
        else:
            sub_queries = [query_request.query]  # Fallback
        
        # Use embedding system for retrieval
        embedding_system = get_embedding_system()
        relevant_docs = []
        
        if embedding_system:
            # Get embeddings and search
            embeddings = await asyncio.to_thread(
                embedding_system.get_embeddings,
                sub_queries
            )
            
            relevant_docs = await asyncio.to_thread(
                embedding_system.similarity_search,
                embeddings,
                top_k=5
            )
        
        # Generate response (you'll need to implement this based on your LLM integration)
        response_text = await generate_response_from_docs(
            query_request.query, 
            relevant_docs, 
            query_request.language
        )
        
        # Extract sources
        sources = [
            Source(
                title=doc.get('title', 'Agricultural Resource'),
                url=doc.get('url', '#'),
                relevance=doc.get('score', 0.8),
                content_preview=doc.get('content', '')[:200] + "..."
            )
            for doc in relevant_docs[:3]
        ]
        
        return QueryResponse(
            answer=response_text,
            sources=sources,
            confidence=0.85,
            sub_queries=sub_queries
        )
        
    except Exception as e:
        logger.error(f"Backend processing failed: {e}")
        # Fallback to mock response
        return await get_enhanced_mock_response(query_request)

async def generate_response_from_docs(query: str, docs: List[Dict], language: str) -> str:
    """
    Generate response from retrieved documents
    This is where you'd integrate your LLM (Gemma3, DeepSeek, etc.)
    """
    try:
        # TODO: Integrate with your actual LLM
        # from your_llm_module import generate_response
        # response = generate_response(query, docs, language)
        
        # For now, create a structured response from docs
        if docs:
            response = f"рдЖрдкрдХреЗ рдкреНрд░рд╢реНрди '{query}' рдХреЗ рдЖрдзрд╛рд░ рдкрд░:\n\n"
            
            for i, doc in enumerate(docs[:3], 1):
                content = doc.get('content', '')[:300]
                response += f"{i}. {content}...\n\n"
            
            response += "рдпрд╣ рдЬрд╛рдирдХрд╛рд░реА рд╣рдорд╛рд░реЗ рдХреГрд╖рд┐ рдбреЗрдЯрд╛рдмреЗрд╕ рд╕реЗ рдкреНрд░рд╛рдкреНрдд рдХреА рдЧрдИ рд╣реИ рдЬрд┐рд╕рдореЗрдВ 15,000+ рдкреНрд░рдорд╛рдгрд┐рдд рд╕реНрд░реЛрдд рд╣реИрдВред"
            return response
        else:
            return await get_fallback_response(query, language)
            
    except Exception as e:
        logger.error(f"Response generation failed: {e}")
        return await get_fallback_response(query, language)

async def get_fallback_response(query: str, language: str) -> str:
    """Fallback response when systems fail"""
    return f"рдЖрдкрдХрд╛ рдкреНрд░рд╢реНрди '{query}' рдкреНрд░рд╛рдкреНрдд рд╣реБрдЖ рд╣реИред рд╣рдорд╛рд░реЗ AI рд╕рд┐рд╕реНрдЯрдо рдореЗрдВ рдЕрд╕реНрдерд╛рдпреА рд╕рдорд╕реНрдпрд╛ рд╣реИред рдХреГрдкрдпрд╛ рдХреБрдЫ рджреЗрд░ рдмрд╛рдж рджреЛрдмрд╛рд░рд╛ рдХреЛрд╢рд┐рд╢ рдХрд░реЗрдВред"

async def get_enhanced_mock_response(query_request: QueryRequest) -> QueryResponse:
    """Enhanced mock responses for testing"""
    query = query_request.query.lower()
    
    # Simulate processing delay
    await asyncio.sleep(0.5)
    
    if any(word in query for word in ['рдореМрд╕рдо', 'weather', 'рдмрд╛рд░рд┐рд╢', 'рддрд╛рдкрдорд╛рди']):
        return QueryResponse(
            answer="""**рдЖрдЬ рдХрд╛ рдореМрд╕рдо рд╡рд┐рд╢реНрд▓реЗрд╖рдг:**

ЁЯМбя╕П **рддрд╛рдкрдорд╛рди:** 25┬░C (рдиреНрдпреВрдирддрдо) - 33┬░C (рдЕрдзрд┐рдХрддрдо)
ЁЯМзя╕П **рдмрд╛рд░рд┐рд╢:** рдЕрдЧрд▓реЗ 3 рджрд┐рди 70% рд╕рдВрднрд╛рд╡рдирд╛
ЁЯТи **рд╣рд╡рд╛:** 15 рдХрд┐рдореА/рдШрдВрдЯрд╛, рджрдХреНрд╖рд┐рдг-рдкреВрд░реНрд╡ рджрд┐рд╢рд╛
тШБя╕П **рдЖрд░реНрджреНрд░рддрд╛:** 75%

**ЁЯМ╛ рдХреГрд╖рд┐ рд╕рд▓рд╛рд╣:**
тАв рд╕рд┐рдВрдЪрд╛рдИ рдХрдо рдХрд░реЗрдВ рдХреНрдпреЛрдВрдХрд┐ рдмрд╛рд░рд┐рд╢ рдХреА рд╕рдВрднрд╛рд╡рдирд╛ рд╣реИ
тАв рдзрд╛рди рдФрд░ рдордХреНрдХрд╛ рдХреА рдмреБрдЖрдИ рдХреЗ рд▓рд┐рдП рдЕрдЪреНрдЫрд╛ рд╕рдордп
тАв рдЦрд╛рдж рдбрд╛рд▓рдиреЗ рд╕реЗ рдмрдЪреЗрдВ, рдмрд╛рд░рд┐рд╢ рдХреЗ рдмрд╛рдж рдбрд╛рд▓реЗрдВ
тАв рдлрд╕рд▓ рдореЗрдВ рдбреНрд░реЗрдиреЗрдЬ рдХреА рд╡реНрдпрд╡рд╕реНрдерд╛ рдХрд░реЗрдВ

**тЪая╕П рд╕рд╛рд╡рдзрд╛рдиреА:** рддреЗрдЬрд╝ рд╣рд╡рд╛ рд╕реЗ рд╕рдмреНрдЬрд┐рдпреЛрдВ рдХреА рдлрд╕рд▓ рдХреЛ рдиреБрдХрд╕рд╛рди рд╣реЛ рд╕рдХрддрд╛ рд╣реИред""",
            sources=[
                Source(title="IMD Weather Report", url="#", relevance=0.95),
                Source(title="ICAR Weather Advisory", url="#", relevance=0.88),
                Source(title="Agricultural Meteorology", url="#", relevance=0.82)
            ],
            confidence=0.92,
            sub_queries=["рдореМрд╕рдо рдкреВрд░реНрд╡рд╛рдиреБрдорд╛рди", "рдХреГрд╖рд┐ рдореМрд╕рдо рд╕рд▓рд╛рд╣", "рдмрд╛рд░рд┐рд╢ рдХреГрд╖рд┐ рдкреНрд░рднрд╛рд╡"]
        )
    
    elif any(word in query for word in ['рдзрд╛рди', 'rice', 'рдЪрд╛рд╡рд▓', 'рдкреАрд▓реЗ рдкрддреНрддреЗ']):
        return QueryResponse(
            answer="""**рдзрд╛рди рдореЗрдВ рдкреАрд▓реЗ рдкрддреНрддреЛрдВ рдХреА рд╕рдорд╕реНрдпрд╛ рдХрд╛ рд╕рдорд╛рдзрд╛рди:**

ЁЯФН **рдореБрдЦреНрдп рдХрд╛рд░рдг:**

1я╕ПтГг **рдирд╛рдЗрдЯреНрд░реЛрдЬрди рдХреА рдХрдореА** (рд╕рдмрд╕реЗ рдЖрдо)
   тАв рд▓рдХреНрд╖рдг: рдкреБрд░рд╛рдиреЗ рдкрддреНрддреЗ рдкрд╣рд▓реЗ рдкреАрд▓реЗ рд╣реЛрддреЗ рд╣реИрдВ
   тАв рд╕рдорд╛рдзрд╛рди: 25-30 рдХрд┐рд▓реЛ рдпреВрд░рд┐рдпрд╛ рдкреНрд░рддрд┐ рдПрдХрдбрд╝

2я╕ПтГг **рдЬрд▓рднрд░рд╛рд╡** 
   тАв рд▓рдХреНрд╖рдг: рдЬрдбрд╝реЛрдВ рдХрд╛ рдХрд╛рд▓рд╛ рд╣реЛрдирд╛
   тАв рд╕рдорд╛рдзрд╛рди: рдбреНрд░реЗрдиреЗрдЬ рдмрдирд╛рдПрдВ, рд╕рд┐рдВрдЪрд╛рдИ рдХрдо рдХрд░реЗрдВ

3я╕ПтГг **рдЖрдпрд░рди рдХреА рдХрдореА**
   тАв рд▓рдХреНрд╖рдг: рдирдИ рдкрддреНрддрд┐рдпреЛрдВ рдореЗрдВ рдкреАрд▓рд╛рдкрди
   тАв рд╕рдорд╛рдзрд╛рди: рдЖрдпрд░рди рд╕рд▓реНрдлреЗрдЯ рдХрд╛ рдЫрд┐рдбрд╝рдХрд╛рд╡ (2 рдЧреНрд░рд╛рдо/рд▓реАрдЯрд░)

4я╕ПтГг **рдмреИрдХреНрдЯреАрд░рд┐рдпрд▓ рд▓реАрдл рдмреНрд▓рд╛рдЗрдЯ**
   тАв рд▓рдХреНрд╖рдг: рдкрддреНрддрд┐рдпреЛрдВ рдХреЗ рдХрд┐рдирд╛рд░реЗ рдЭреБрд▓рд╕реЗ рд╣реБрдП
   тАв рд╕рдорд╛рдзрд╛рди: рдХреЙрдкрд░ рдСрдХреНрд╕реАрдХреНрд▓реЛрд░рд╛рдЗрдб рдХрд╛ рд╕реНрдкреНрд░реЗ

ЁЯТК **рддреБрд░рдВрдд рдЙрдкрдЪрд╛рд░:**
тАв рдлрд╝реЙрд╕реНрдлрд╝реЛрд░рд╕ рдФрд░ рдкреЛрдЯрд╛рд╢ рдХреА рдорд╛рддреНрд░рд╛ рдмрдврд╝рд╛рдПрдВ
тАв рдиреАрдо рдХрд╛ рддреЗрд▓ рдорд┐рд▓рд╛рдХрд░ рдЫрд┐рдбрд╝рдХрд╛рд╡ рдХрд░реЗрдВ
тАв рдорд┐рдЯреНрдЯреА рдХреА рдЬрд╛рдВрдЪ рдХрд░рд╛рдПрдВ""",
            sources=[
                Source(title="ICAR Rice Cultivation Guide", url="#", relevance=0.94),
                Source(title="Plant Pathology Research", url="#", relevance=0.89),
                Source(title="Rice Disease Management", url="#", relevance=0.85)
            ],
            confidence=0.96,
            sub_queries=["рдзрд╛рди рдкреАрд▓реЗ рдкрддреНрддреЗ рдХрд╛рд░рдг", "рдзрд╛рди рд░реЛрдЧ рдирд┐рджрд╛рди", "рдирд╛рдЗрдЯреНрд░реЛрдЬрди рдХреА рдХрдореА рдзрд╛рди рдореЗрдВ"]
        )
    
    # Add more mock responses as needed...
    
    return QueryResponse(
        answer=f"""рдЖрдкрдХрд╛ рдкреНрд░рд╢реНрди "{query_request.query}" рджрд┐рд▓рдЪрд╕реНрдк рд╣реИред 

**ЁЯМ╛ IndicAgri рдореЗрдВ рдЙрдкрд▓рдмреНрдз рд╕реЗрд╡рд╛рдПрдВ:**

ЁЯУК **рдлрд╕рд▓ рд╕рдВрдмрдВрдзреА рдЬрд╛рдирдХрд╛рд░реА:**
тАв рдмреБрдЖрдИ, рдХрдЯрд╛рдИ рдХрд╛ рд╕рд╣реА рд╕рдордп
тАв рдмреАрдЬ, рдЦрд╛рдж, рдХреАрдЯрдирд╛рд╢рдХ рдХреА рд╕рд▓рд╛рд╣
тАв рд░реЛрдЧ-рдХреАрдЯ рдХреА рдкрд╣рдЪрд╛рди рдФрд░ рдЗрд▓рд╛рдЬ

ЁЯМдя╕П **рдореМрд╕рдо рдЖрдзрд╛рд░рд┐рдд рд╕рд▓рд╛рд╣:**
тАв 7 рджрд┐рди рдХрд╛ рдореМрд╕рдо рдкреВрд░реНрд╡рд╛рдиреБрдорд╛рди
тАв рд╕рд┐рдВрдЪрд╛рдИ рдХреА рд╕рд▓рд╛рд╣
тАв рдкреНрд░рд╛рдХреГрддрд┐рдХ рдЖрдкрджрд╛ рд╕реЗ рдмрдЪрд╛рд╡

ЁЯТ░ **рдмрд╛рдЬрд╝рд╛рд░ рдХреА рдЬрд╛рдирдХрд╛рд░реА:**
тАв рдЖрдЬ рдХреЗ рднрд╛рд╡ рдФрд░ рд░реБрдЭрд╛рди
тАв рдмреЗрдЪрдиреЗ рдХрд╛ рд╕рд╣реА рд╕рдордп
тАв рд╕рд░рдХрд╛рд░реА рдЦрд░реАрдж рдХреЗрдВрджреНрд░ рдХреА рдЬрд╛рдирдХрд╛рд░реА

ЁЯПЫя╕П **рд╕рд░рдХрд╛рд░реА рдпреЛрдЬрдирд╛рдПрдВ:**
тАв рд╕рдмреНрд╕рд┐рдбреА рдФрд░ рдмреАрдорд╛ рдпреЛрдЬрдирд╛рдПрдВ
тАв рдЖрд╡реЗрджрди рдХреА рдкреНрд░рдХреНрд░рд┐рдпрд╛
тАв рдкрд╛рддреНрд░рддрд╛ рдХреА рдЬрд╛рдВрдЪ

рдХреГрдкрдпрд╛ рдЕрдзрд┐рдХ рд╡рд┐рд╢рд┐рд╖реНрдЯ рдкреНрд░рд╢реНрди рдкреВрдЫреЗрдВред""",
        sources=[
            Source(title="IndicAgri Knowledge Base", url="#", relevance=0.75)
        ],
        confidence=0.78,
        sub_queries=[query_request.query]
    )

@app.post("/api/v1/voice")
async def process_voice(
    audio: UploadFile = File(...),
    language: str = "hi"
):
    """Process voice input (placeholder for future implementation)"""
    try:
        # TODO: Integrate with your voice processing module
        # from voice_to_text.voice_to_text.indic_conformer import process_audio
        # transcription = process_audio(audio, language)
        
        # For now, return a mock transcription
        return {
            "transcription": "рдпрд╣рд╛рдБ рдЖрдкрдХреА рдЖрд╡рд╛рдЬрд╝ рдХрд╛ рдЯреЗрдХреНрд╕реНрдЯ рд╣реЛрдЧрд╛",
            "confidence": 0.85,
            "language_detected": language
        }
    except Exception as e:
        logger.error(f"Voice processing error: {e}")
        raise HTTPException(status_code=500, detail="Voice processing failed")

# Error handlers
@app.exception_handler(404)
async def not_found_handler(request: Request, exc: HTTPException):
    """Redirect 404s to home page"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.exception_handler(500)
async def server_error_handler(request: Request, exc: HTTPException):
    """Handle server errors gracefully"""
    logger.error(f"Server error: {exc.detail}")
    return JSONResponse(
        status_code=500,
        content={"error": "Internal server error", "detail": "Something went wrong"}
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
