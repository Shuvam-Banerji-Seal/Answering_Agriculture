# Sub-Query Generation Configuration

# Model Configuration
model:
  # Choose implementation: 'ollama' or 'huggingface'
  implementation: 'ollama'
  
  # Ollama Configuration
  ollama:
    model_name: 'gemma2:2b'
    base_url: 'http://localhost:11434'
    timeout: 30
  
  # HuggingFace Configuration
  huggingface:
    model_id: 'google/gemma-2-2b-it'
    use_quantization: false
    quantization_bits: 8
    device: 'auto'
    torch_dtype: 'bfloat16'

# Generation Parameters
generation:
  temperature: 0.7
  max_new_tokens: 1000
  do_sample: true
  num_sub_queries: 5

# Sub-Query Generation Strategy
strategy:
  variations:
    - synonym_variation
    - technical_reformulation
    - simplified_version
    - context_expansion
    - perspective_shift
  
  rag_optimization:
    sentence_format: true
    keyword_focus: true
    max_length_per_query: 2

# Logging
logging:
  level: 'INFO'
  file: 'sub_query_generation.log'